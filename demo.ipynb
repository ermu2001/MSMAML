{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ms12416/anaconda3/envs/AudioMMAML/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from maml.datasets.esc50 import ESC50MetaDataset\n",
    "from main import parse_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_training = False\n",
    "args = parse_args([\n",
    "    '--dataset', 'esc50',\n",
    "    '--num-batches', '6000',\n",
    "    '--output-folder', 'mmaml_5mode_5w1s',\n",
    "    '--verbose',\n",
    "    '--model-type', 'gated_conv_1d',\n",
    "    '--embedding-type', 'ConvGRU1d',\n",
    "    '--num-workers', '0',\n",
    "    '--eval',\n",
    "    '--checkpoint', 'train_dir/mmaml_5mode_5w1s/maml_gated_conv_1d_6000.pt',    \n",
    "])\n",
    "\n",
    "args.num_sample_embedding = min(args.num_sample_embedding, args.num_batches)\n",
    "\n",
    "# computer embedding dims\n",
    "num_gated_conv_layers = 4\n",
    "if args.embedding_dims == 0:\n",
    "    args.embedding_dims = []\n",
    "    for i in range(num_gated_conv_layers):\n",
    "        embedding_dim = args.num_channels*2**i\n",
    "        if args.condition_type == 'affine':\n",
    "            embedding_dim *= 2\n",
    "        args.embedding_dims.append(embedding_dim)\n",
    "assert not (args.mmaml_model and args.maml_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ESC50MetaDataset(\n",
    "    root='data',\n",
    "    audio_side_len=32,\n",
    "    audio_channel=1,\n",
    "    num_classes_per_batch=args.num_classes_per_batch,\n",
    "    num_samples_per_class=args.num_samples_per_class,\n",
    "    num_total_batches=args.num_batches,\n",
    "    num_val_samples=args.num_val_samples,\n",
    "    meta_batch_size=args.meta_batch_size,\n",
    "    train=is_training,\n",
    "    num_train_classes=args.num_train_classes,\n",
    "    num_workers=args.num_workers,\n",
    "    device=args.device)\n",
    "classmap = dataset.classname\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "collect_accuracies = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maml.models.conv_embedding_1d_model import ConvEmbeddingOneDimensionalModel\n",
    "from maml.models.gated_conv_net_1d import GatedConv1dModel\n",
    "\n",
    "model = GatedConv1dModel(\n",
    "    input_channels=dataset.input_size[0],\n",
    "    output_size=dataset.output_size,\n",
    "    num_channels=args.num_channels,\n",
    "    img_side_len=dataset.input_size[1],\n",
    "    use_max_pool=args.use_max_pool,\n",
    "    verbose=args.verbose)\n",
    "\n",
    "embedding_model = ConvEmbeddingOneDimensionalModel(\n",
    "        input_size=np.prod(dataset.input_size),\n",
    "        output_size=dataset.output_size,\n",
    "        embedding_dims=args.embedding_dims,\n",
    "        hidden_size=args.embedding_hidden_size,\n",
    "        num_layers=args.embedding_num_layers,\n",
    "        convolutional=args.conv_embedding,\n",
    "        num_conv=args.num_conv_embedding_layer,\n",
    "        num_channels=args.num_channels,\n",
    "        rnn_aggregation=(not args.no_rnn_aggregation),\n",
    "        embedding_pooling=args.embedding_pooling,\n",
    "        batch_norm=args.conv_embedding_batch_norm,\n",
    "        avgpool_after_conv=args.conv_embedding_avgpool_after_conv,\n",
    "        linear_before_rnn=args.linear_before_rnn,\n",
    "        num_sample_embedding=args.num_sample_embedding,\n",
    "        sample_embedding_file=args.sample_embedding_file+'.'+args.sample_embedding_file_type,\n",
    "        img_size=dataset.input_size,\n",
    "        verbose=args.verbose)\n",
    "embedding_parameters = list(embedding_model.parameters())\n",
    "\n",
    "\n",
    "if args.checkpoint != '':\n",
    "    checkpoint = torch.load(args.checkpoint)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(args.device)\n",
    "    if embedding_model: \n",
    "        embedding_model.load_state_dict(checkpoint['embedding_model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maml.metalearner import MetaLearner\n",
    "\n",
    "optimizers = []\n",
    "meta_learner = MetaLearner(\n",
    "    model, embedding_model, optimizers, fast_lr=args.fast_lr,\n",
    "    loss_func=loss_func, first_order=args.first_order,\n",
    "    num_updates=args.num_updates,\n",
    "    inner_loop_grad_clip=args.inner_loop_grad_clip,\n",
    "    collect_accuracies=collect_accuracies, device=args.device,\n",
    "    alternating=args.alternating, embedding_schedule=args.embedding_schedule,\n",
    "    classifier_schedule=args.classifier_schedule, embedding_grad_clip=args.embedding_grad_clip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def adapt(train_tasks):\n",
    "    adapted_params = []\n",
    "    embeddings_list = []\n",
    "    # import pdb; pdb.set_trace();\n",
    "    for task in train_tasks:\n",
    "        params = model.param_dict\n",
    "        embeddings = None\n",
    "        if embedding_model:\n",
    "            embeddings = embedding_model(task)\n",
    "        for i in range(args.num_updates):\n",
    "            preds = model(task, params=params, embeddings=embeddings)\n",
    "            loss = loss_func(preds, task.y)\n",
    "            params = update_params(loss, params=params)\n",
    "        adapted_params.append(params)\n",
    "        embeddings_list.append(embeddings)\n",
    "\n",
    "    return adapted_params, embeddings_list\n",
    "\n",
    "\n",
    "def step(adapted_params_list, embeddings_list, val_tasks):\n",
    "    post_update_losses = []\n",
    "    pred_list = []\n",
    "    for adapted_params, embeddings, task in zip(adapted_params_list, embeddings_list, val_tasks):\n",
    "        preds = model(task, params=adapted_params, embeddings=embeddings)\n",
    "        pred_list.append(preds)\n",
    "        loss = loss_func(preds, task.y)\n",
    "        post_update_losses.append(loss)\n",
    "\n",
    "    mean_loss = torch.mean(torch.stack(post_update_losses))\n",
    "    return mean_loss, pred_list\n",
    "\n",
    "def update_params(loss, params):\n",
    "    \"\"\"Apply one step of gradient descent on the loss function `loss`,\n",
    "    with step-size `self._fast_lr`, and returns the updated parameters.\n",
    "    \"\"\"\n",
    "    create_graph = not args.first_order\n",
    "    grads = torch.autograd.grad(loss, params.values(),\n",
    "                                create_graph=create_graph, allow_unused=True)\n",
    "    for (name, param), grad in zip(params.items(), grads):\n",
    "        if args.inner_loop_grad_clip > 0 and grad is not None:\n",
    "            grad = grad.clamp(min=-args.inner_loop_grad_clip,\n",
    "                                max=args.inner_loop_grad_clip)\n",
    "        if grad is not None:\n",
    "            params[name] = param - args.fast_lr * grad\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "def tensor2wavfile(tensor, filename, sample_rate=44100):\n",
    "    # Ensure tensor is on CPU\n",
    "    if tensor.ndim > 1:\n",
    "        raise ValueError(f'tensor with dim {tensor.ndim}')\n",
    "    tensor = tensor.cpu().numpy()\n",
    "    sf.write(filename, tensor, sample_rate,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Emb Model ========\n",
      "input size: torch.Size([5, 1, 80000])\n",
      "conv1: torch.Size([5, 32, 16000])\n",
      "bn1: torch.Size([5, 32, 16000])\n",
      "relu1: torch.Size([5, 32, 16000])\n",
      "conv2: torch.Size([5, 64, 3200])\n",
      "bn2: torch.Size([5, 64, 3200])\n",
      "relu2: torch.Size([5, 64, 3200])\n",
      "conv3: torch.Size([5, 128, 640])\n",
      "bn3: torch.Size([5, 128, 640])\n",
      "relu3: torch.Size([5, 128, 640])\n",
      "conv4: torch.Size([5, 256, 128])\n",
      "bn4: torch.Size([5, 256, 128])\n",
      "relu4: torch.Size([5, 256, 128])\n",
      "reshape to: torch.Size([5, 256, 128])\n",
      "reduce mean: torch.Size([5, 256])\n",
      "fc: torch.Size([1, 128, 5])\n",
      "reshape after avgpool: torch.Size([1, 128])\n",
      "emb vec 1 size: torch.Size([1, 64])\n",
      "emb vec 2 size: torch.Size([1, 128])\n",
      "emb vec 3 size: torch.Size([1, 256])\n",
      "emb vec 4 size: torch.Size([1, 512])\n",
      "===========================\n",
      "========== Model ==========\n",
      "input size: torch.Size([5, 1, 80000])\n",
      "layer1_conv: torch.Size([5, 32, 16000])\n",
      "layer1_bn: torch.Size([5, 32, 16000])\n",
      "layer1_condition: torch.Size([5, 32, 16000])\n",
      "layer1_relu: torch.Size([5, 32, 16000])\n",
      "layer2_conv: torch.Size([5, 64, 3200])\n",
      "layer2_bn: torch.Size([5, 64, 3200])\n",
      "layer2_condition: torch.Size([5, 64, 3200])\n",
      "layer2_relu: torch.Size([5, 64, 3200])\n",
      "layer3_conv: torch.Size([5, 128, 640])\n",
      "layer3_bn: torch.Size([5, 128, 640])\n",
      "layer3_condition: torch.Size([5, 128, 640])\n",
      "layer3_relu: torch.Size([5, 128, 640])\n",
      "layer4_conv: torch.Size([5, 256, 128])\n",
      "layer4_bn: torch.Size([5, 256, 128])\n",
      "layer4_condition: torch.Size([5, 256, 128])\n",
      "layer4_relu: torch.Size([5, 256, 128])\n",
      "reshape to: torch.Size([5, 256, 128])\n",
      "reduce mean: torch.Size([5, 256])\n",
      "logits size: torch.Size([5, 5])\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i, (train_tasks, val_tasks) in enumerate(iter(dataset), start=1):\n",
    "    # pre_train_measurements, adapted_params, embeddings = meta_learner.adapt(train_tasks) # train_tasks measurements before meta updates\n",
    "    # post_val_measurements = meta_learner.step(adapted_params, embeddings, val_tasks, is_training) # val_tasks measurements after meta updates\n",
    "    adapted_params, embeddings = adapt(train_tasks)\n",
    "    mean_loss, pred_list = step(adapted_params, embeddings, val_tasks)\n",
    "    results.append((train_tasks, val_tasks, pred_list))\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 80000])\n",
      "torch.Size([5])\n",
      "torch.Size([75, 1, 80000])\n",
      "torch.Size([75])\n",
      "torch.Size([75, 5])\n"
     ]
    }
   ],
   "source": [
    "print(train_tasks[0].x.shape)\n",
    "print(train_tasks[0].y.shape)\n",
    "print(val_tasks[0].x.shape)\n",
    "print(val_tasks[0].y.shape)\n",
    "print(pred_list[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "output_dir='test'\n",
    "for result in results:\n",
    "    train_tasks, val_tasks, pred_list = result\n",
    "    for i, (train_task, val_task, preds) in enumerate(zip(*result)):\n",
    "        # one task\n",
    "        task_dir = osp.join(output_dir, f'task_{i}')\n",
    "        train_dir = osp.join(task_dir, 'train')\n",
    "        val_dir = osp.join(task_dir, 'val')\n",
    "        os.makedirs(train_dir, exist_ok=True)\n",
    "        os.makedirs(val_dir, exist_ok=True)\n",
    "        preds = preds.argmax(dim=-1)\n",
    "        # print(train_task.x.shape)\n",
    "        # print(train_task.y.shape)\n",
    "        # print(val_task.x.shape)\n",
    "        # print(val_task.y.shape)\n",
    "        # print(preds.shape)\n",
    "        sample_rate = 16000\n",
    "        for i, (audio_tensor, y, gt) in enumerate(zip(train_task.x, train_task.y,train_task.gt)):\n",
    "            tensor2wavfile(audio_tensor[0], osp.join(train_dir, f'train_audio_gt{gt}_id{i}_category_{classmap[int(gt)]}.wav'), sample_rate=sample_rate)\n",
    "        for i, (audio_tensor, y, pred, gt) in enumerate(zip(val_task.x, val_task.y, preds, val_task.gt)):\n",
    "            tensor2wavfile(audio_tensor[0], osp.join(val_dir, f'val_audio_gt{gt}_pred{pred}_id{i}_category_{classmap[int(gt)]}.wav'), sample_rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
