{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from maml.datasets.esc50 import ESC50MetaDataset\n",
    "# from maml.datasets.cifar100_simulate_1d import Cifar100MetaDataset\n",
    "from maml.datasets.cifar100 import Cifar100MetaDataset\n",
    "from main import parse_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_training = False\n",
    "args = parse_args([\n",
    "    '--dataset', 'multimodal_few_shot',\n",
    "    '--multimodal_few_shot', 'esc50', 'cifar',\n",
    "    '--common-img-side-len', '32',\n",
    "    '--common-img-channel', '3',\n",
    "    '--num-batches', '6000',\n",
    "    '--output-folder', 'mmaml_5mode_5w1s',\n",
    "    '--verbose',\n",
    "    '--model-type', 'gated_conv_1d',\n",
    "    '--embedding-type', 'ConvGRU1d',\n",
    "    '--num-workers', '0',\n",
    "    '--eval',\n",
    "    '--device', 'cuda:1',\n",
    "    # '--checkpoint', 'train_dir/real_multimodal_2mode_5w1s/maml_gated_conv_1d_6000.pt',    \n",
    "])\n",
    "\n",
    "args.num_sample_embedding = min(args.num_sample_embedding, args.num_batches)\n",
    "\n",
    "# computer embedding dims\n",
    "num_gated_conv_layers = 4\n",
    "if args.embedding_dims == 0:\n",
    "    args.embedding_dims = []\n",
    "    for i in range(num_gated_conv_layers):\n",
    "        embedding_dim = args.num_channels*2**i\n",
    "        if args.condition_type == 'affine':\n",
    "            embedding_dim *= 2\n",
    "        args.embedding_dims.append(embedding_dim)\n",
    "assert not (args.mmaml_model and args.maml_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maml.datasets.multimodal_few_shot import MultimodalFewShotDataset\n",
    "\n",
    "\n",
    "dataset_list = []\n",
    "dataset_list.append( ESC50MetaDataset(\n",
    "    root='data',\n",
    "    audio_side_len=32,\n",
    "    audio_channel=1,\n",
    "    num_classes_per_batch=args.num_classes_per_batch,\n",
    "    num_samples_per_class=args.num_samples_per_class,\n",
    "    num_total_batches=args.num_batches,\n",
    "    num_val_samples=args.num_val_samples,\n",
    "    meta_batch_size=args.meta_batch_size,\n",
    "    train=is_training,\n",
    "    num_train_classes=args.num_train_classes,\n",
    "    num_workers=args.num_workers,\n",
    "    device=args.device)\n",
    ")   \n",
    "\n",
    "dataset_list.append(Cifar100MetaDataset(\n",
    "    root='data',\n",
    "    img_side_len=args.common_img_side_len,\n",
    "    img_channel=args.common_img_channel,\n",
    "    num_classes_per_batch=args.num_classes_per_batch,\n",
    "    num_samples_per_class=args.num_samples_per_class,\n",
    "    num_total_batches=args.num_batches,\n",
    "    num_val_samples=args.num_val_samples,\n",
    "    meta_batch_size=args.meta_batch_size,\n",
    "    train=is_training,\n",
    "    num_train_classes=args.num_train_classes,\n",
    "    num_workers=args.num_workers,\n",
    "    device=args.device)\n",
    ")\n",
    "\n",
    "print('Multimodal Few Shot Datasets: {}'.format(\n",
    "    ' '.join([dataset.name for dataset in dataset_list])))\n",
    "dataset = MultimodalFewShotDataset(\n",
    "    dataset_list, \n",
    "    num_total_batches=args.num_batches,\n",
    "    mix_meta_batch=args.mix_meta_batch,\n",
    "    mix_mini_batch=args.mix_mini_batch,\n",
    "    txt_file=args.sample_embedding_file+'.txt' if args.num_sample_embedding > 0 else None,\n",
    "    train=is_training,\n",
    ")\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "collect_accuracies = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTIMODAL DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maml.models.conv_embedding_1d_model import ConvEmbeddingOneDimensionalModel\n",
    "from maml.models.gated_conv_net_1d import GatedConv1dModel\n",
    "\n",
    "model = GatedConv1dModel(\n",
    "    input_channels=dataset.input_size[0],\n",
    "    output_size=dataset.output_size,\n",
    "    num_channels=args.num_channels,\n",
    "    img_side_len=dataset.input_size[1],\n",
    "    use_max_pool=args.use_max_pool,\n",
    "    verbose=args.verbose)\n",
    "\n",
    "embedding_model = ConvEmbeddingOneDimensionalModel(\n",
    "        input_size=np.prod(dataset.input_size),\n",
    "        output_size=dataset.output_size,\n",
    "        embedding_dims=args.embedding_dims,\n",
    "        hidden_size=args.embedding_hidden_size,\n",
    "        num_layers=args.embedding_num_layers,\n",
    "        convolutional=args.conv_embedding,\n",
    "        num_conv=args.num_conv_embedding_layer,\n",
    "        num_channels=args.num_channels,\n",
    "        rnn_aggregation=(not args.no_rnn_aggregation),\n",
    "        embedding_pooling=args.embedding_pooling,\n",
    "        batch_norm=args.conv_embedding_batch_norm,\n",
    "        avgpool_after_conv=args.conv_embedding_avgpool_after_conv,\n",
    "        linear_before_rnn=args.linear_before_rnn,\n",
    "        num_sample_embedding=args.num_sample_embedding,\n",
    "        sample_embedding_file=args.sample_embedding_file+'.'+args.sample_embedding_file_type,\n",
    "        img_size=dataset.input_size,\n",
    "        verbose=args.verbose)\n",
    "embedding_parameters = list(embedding_model.parameters())\n",
    "\n",
    "\n",
    "if args.checkpoint != '':\n",
    "    checkpoint = torch.load(args.checkpoint)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(args.device)\n",
    "    if embedding_model: \n",
    "        embedding_model.load_state_dict(checkpoint['embedding_model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# META LEARNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maml.metalearner import MetaLearner\n",
    "\n",
    "optimizers = []\n",
    "meta_learner = MetaLearner(\n",
    "    model, embedding_model, optimizers, fast_lr=args.fast_lr,\n",
    "    loss_func=loss_func, first_order=args.first_order,\n",
    "    num_updates=args.num_updates,\n",
    "    inner_loop_grad_clip=args.inner_loop_grad_clip,\n",
    "    collect_accuracies=collect_accuracies, device=args.device,\n",
    "    alternating=args.alternating, embedding_schedule=args.embedding_schedule,\n",
    "    classifier_schedule=args.classifier_schedule, embedding_grad_clip=args.embedding_grad_clip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def adapt(train_tasks):\n",
    "    adapted_params = []\n",
    "    embeddings_list = []\n",
    "    # import pdb; pdb.set_trace();\n",
    "    for task in train_tasks:\n",
    "        params = model.param_dict\n",
    "        embeddings = None\n",
    "        if embedding_model:\n",
    "            embeddings = embedding_model(task)\n",
    "        for i in range(args.num_updates):\n",
    "            preds = model(task, params=params, embeddings=embeddings)\n",
    "            loss = loss_func(preds, task.y)\n",
    "            params = update_params(loss, params=params)\n",
    "        adapted_params.append(params)\n",
    "        embeddings_list.append(embeddings)\n",
    "\n",
    "    return adapted_params, embeddings_list\n",
    "\n",
    "\n",
    "def step(adapted_params_list, embeddings_list, val_tasks):\n",
    "    post_update_losses = []\n",
    "    pred_list = []\n",
    "    for adapted_params, embeddings, task in zip(adapted_params_list, embeddings_list, val_tasks):\n",
    "        preds = model(task, params=adapted_params, embeddings=embeddings)\n",
    "        pred_list.append(preds)\n",
    "        loss = loss_func(preds, task.y)\n",
    "        post_update_losses.append(loss)\n",
    "\n",
    "    mean_loss = torch.mean(torch.stack(post_update_losses))\n",
    "    return mean_loss, pred_list\n",
    "\n",
    "def update_params(loss, params):\n",
    "    \"\"\"Apply one step of gradient descent on the loss function `loss`,\n",
    "    with step-size `self._fast_lr`, and returns the updated parameters.\n",
    "    \"\"\"\n",
    "    create_graph = not args.first_order\n",
    "    grads = torch.autograd.grad(loss, params.values(),\n",
    "                                create_graph=create_graph, allow_unused=True)\n",
    "    for (name, param), grad in zip(params.items(), grads):\n",
    "        if args.inner_loop_grad_clip > 0 and grad is not None:\n",
    "            grad = grad.clamp(min=-args.inner_loop_grad_clip,\n",
    "                                max=args.inner_loop_grad_clip)\n",
    "        if grad is not None:\n",
    "            params[name] = param - args.fast_lr * grad\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import PIL.Image\n",
    "from matplotlib import pyplot as plt\n",
    "def tensor2wavfile(tensor, filename, sample_rate=44100):\n",
    "    # Ensure tensor is on CPU\n",
    "    if tensor.ndim > 2:\n",
    "        raise ValueError(f'tensor with dim {tensor.ndim}')\n",
    "    elif tensor.ndim == 2:\n",
    "        tensor = tensor.T # multi channel audio\n",
    "    tensor = tensor.cpu().numpy()\n",
    "    sf.write(filename + '.wav', tensor, sample_rate,)\n",
    "\n",
    "def tensor2jpgfile(tensor, filename, output_shape=(32, 32)):\n",
    "    # Ensure tensor is on CPU\n",
    "    if tensor.ndim > 3:\n",
    "        raise ValueError(f'tensor with dim {tensor.ndim}')\n",
    "    img_pil = to_pil_image(tensor)\n",
    "    img_pil = img_pil.resize(output_shape)\n",
    "    img_pil.save(filename + '.jpg')\n",
    "    \n",
    "def task_tensor2file(taskname, tensor, filename):\n",
    "    if taskname in ['CIFAR1001d', ]:\n",
    "        h = int(np.sqrt(tensor.shape[-1] // 3))\n",
    "        tensor = tensor.view(3, h, tensor.shape[-1] // 3 // h)\n",
    "        tensor2jpgfile(tensor, filename)\n",
    "    elif taskname in [ 'FC100']:\n",
    "        tensor2jpgfile(tensor, filename)\n",
    "    elif taskname in ['ESC50']:\n",
    "        tensor2wavfile(tensor, filename, sample_rate=16000)\n",
    "    else:\n",
    "        raise ValueError(f'not valid task name {taskname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i, (train_tasks, val_tasks) in enumerate(iter(dataset), start=1):\n",
    "    # pre_train_measurements, adapted_params, embeddings = meta_learner.adapt(train_tasks) # train_tasks measurements before meta updates\n",
    "    # post_val_measurements = meta_learner.step(adapted_params, embeddings, val_tasks, is_training) # val_tasks measurements after meta updates\n",
    "    adapted_params, embeddings = adapt(train_tasks)\n",
    "    mean_loss, pred_list = step(adapted_params, embeddings, val_tasks)\n",
    "    results.append((train_tasks, val_tasks, pred_list))\n",
    "\n",
    "    if i > 2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_tasks[0].x.shape)\n",
    "print(train_tasks[0].y.shape)\n",
    "print(val_tasks[0].x.shape)\n",
    "print(val_tasks[0].y.shape)\n",
    "print(pred_list[0].shape)\n",
    "print(type(results[0][0][0].task_info))\n",
    "print(results[0][0][0].task_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "output_dir='test'\n",
    "for j, result in enumerate(results):\n",
    "    result_output_dir = osp.join(output_dir, f'result{j}')\n",
    "    train_tasks, val_tasks, pred_list = result\n",
    "    for i, (train_task, val_task, preds) in enumerate(zip(*result)):\n",
    "        # one task\n",
    "        print(train_task.task_info)\n",
    "        task_dir = osp.join(result_output_dir, f'task_{i}')\n",
    "        train_dir = osp.join(task_dir, 'train')\n",
    "        val_dir = osp.join(task_dir, 'val')\n",
    "        os.makedirs(train_dir, exist_ok=True)\n",
    "        os.makedirs(val_dir, exist_ok=True)\n",
    "        preds = preds.argmax(dim=-1)\n",
    "        # print(train_task.x.shape)\n",
    "        # print(train_task.y.shape)\n",
    "        # print(val_task.x.shape)\n",
    "        # print(val_task.y.shape)\n",
    "        # print(preds.shape)\n",
    "        sample_rate = 16000\n",
    "        gt2class = {}\n",
    "        for i, (tensor, y, gt) in enumerate(zip(train_task.x, train_task.y, train_task.gt)):\n",
    "            class_name = dataset.get_class_name(train_task.task_info, int(gt))\n",
    "            task_tensor2file(train_task.task_info, tensor, osp.join(train_dir, f'train_{train_task.task_info}_gt{y}_category_{class_name}_id{i}'))\n",
    "            gt2class[y.tolist()] = class_name\n",
    "        for i, (tensor, y, pred) in enumerate(zip(val_task.x, val_task.y, preds)):\n",
    "            class_name = gt2class[pred.tolist()]\n",
    "            task_tensor2file(val_task.task_info, tensor, osp.join(val_dir, f'val_{val_task.task_info}_gt{y}_pred{pred}_category_{class_name}_id{i}'),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = torch.clip(tensor[0] * 255, 0, 255)\n",
    "# temp = temp.to(dtype=torch.uint8)\n",
    "# temp = temp.reshape(3, 163, 163).cpu().numpy()[::-1, ...].transpose(1, 2, 0)\n",
    "# PIL.Image.fromarray(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_task.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
